{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4276ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b263d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  author={ node_id=[90941] },\n",
       "  paper={\n",
       "    node_id=[63854],\n",
       "    x=[63854, 256],\n",
       "  },\n",
       "  (author, writes, paper)={ edge_index=[2, 320187] },\n",
       "  (paper, rev_writes, author)={ edge_index=[2, 320187] }\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets start by loading the data\n",
    "\n",
    "data = torch.load(\"data/hetero_data_no_coauthor.pt\", weights_only=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b1ac43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  author={ node_id=[90941] },\n",
       "  paper={\n",
       "    node_id=[63854],\n",
       "    x=[63854, 256],\n",
       "  },\n",
       "  (author, writes, paper)={\n",
       "    edge_index=[2, 179306],\n",
       "    edge_label=[76845],\n",
       "    edge_label_index=[2, 76845],\n",
       "  },\n",
       "  (paper, rev_writes, author)={ edge_index=[2, 179306] }\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the Train, Val, Test Split\n",
    "# training (80%), validation (10%), and testing edges (10%).\n",
    "# Across the training edges, we use 70% of edges for message passing,\n",
    "# and 30% of edges for supervision. (This is from a tutorial by PyG, we can change this later)\n",
    "# We further want to generate fixed negative edges for evaluation with a ratio of 2:1. (Again a Hyperparameter we can tune later)\n",
    "# Negative edges during training will be generated on-the-fly (How?, again this is from the tutorial, need to check later)\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1, # Validation set percentage\n",
    "    num_test=0.1, # Test set percentage\n",
    "    disjoint_train_ratio=0.3, # Percentage of training edges used for supervision, these will not be used for message passing\n",
    "    neg_sampling_ratio=2.0, # Ratio of negative to positive edges for validation and testing, dont know how this is related to `add_negative_train_samples`, need to check later\n",
    "    add_negative_train_samples=False, # AYYY NO idea, why this set to False, but somehow it works worse with True ???, Need it investigate later, Prolly because we do LinkNeighborLoader which samples neg edges for us?\n",
    "    edge_types=(\"author\", \"writes\", \"paper\"), # Any ways, these are the edge types we want to predict\n",
    "    rev_edge_types=(\"paper\", \"rev_writes\", \"author\"), # Reverse edge types, so we dont accidentally bleed information into validation/test set\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5452de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first hop, we sample at most 20 neighbors.\n",
    "# In the second hop, we sample at most 10 neighbors.\n",
    "# In addition, during training, we want to sample negative edges on-the-fly with\n",
    "# a ratio of 2:1.\n",
    "# We can make use of the `loader.LinkNeighborLoader` from PyG:\n",
    "\n",
    "# This loader is actually SAMPLING the full graph, by first sampling 64 random nodes then 32 neighbors of each node previously sampled node to create a sparse subgraph etc...\n",
    "# We should be able to load the graph fully into memory, but how would one train that?\n",
    "# We could probably use the previous random link split to do full batch training, but somehow we would not sample random negative edges then?\n",
    "# Need to check different loaders which sample the full graph and then do negative sampling on-the-fly\n",
    "edge_label_index = train_data[\"author\", \"writes\", \"paper\"].edge_label_index\n",
    "edge_label = train_data[\"author\", \"writes\", \"paper\"].edge_label\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[64, 32, 16],\n",
    "    neg_sampling_ratio=2.0,\n",
    "    edge_label_index=((\"author\", \"writes\", \"paper\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple 3 hop GNN\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(\n",
    "            hidden_channels,\n",
    "            hidden_channels,\n",
    "            aggr=\"mean\",\n",
    "            project=False,\n",
    "        )\n",
    "        self.conv2 = SAGEConv(\n",
    "            hidden_channels,\n",
    "            hidden_channels,\n",
    "            aggr=\"mean\",\n",
    "            project=False,\n",
    "        )\n",
    "        self.conv3 = SAGEConv(\n",
    "            hidden_channels,\n",
    "            hidden_channels,\n",
    "            aggr=\"mean\",\n",
    "            project=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Our final classifier applies the dot-product between source and destination\n",
    "# node embeddings to derive edge-level predictions:\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        x_user: torch.Tensor,\n",
    "        x_movie: torch.Tensor,\n",
    "        edge_label_index: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_user = x_user[edge_label_index[0]]\n",
    "        edge_feat_movie = x_movie[edge_label_index[1]]\n",
    "        return (edge_feat_user * edge_feat_movie).sum(dim=-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels: int, data: HeteroData):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "\n",
    "        # Convert GNN model into a heterogeneous variant:\n",
    "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "        \n",
    "        # Instantiate link classifier:\n",
    "        self.classifier = Classifier()\n",
    "\n",
    "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
    "\n",
    "        # Set the initial user embeddings to all ones for all authors\n",
    "        # This makes sure the graph can generalize to unseen authors during inference\n",
    "        author_embedding = torch.ones(\n",
    "            (data[\"author\"].num_nodes, self.hidden_channels),\n",
    "            device=data[\"paper\"].x.device,\n",
    "        )\n",
    "\n",
    "        # Extract paper embeddings from the data object\n",
    "        paper_embedding = data[\"paper\"].x\n",
    "\n",
    "        # Noew we can create the x_dict required for the GNN\n",
    "        x_dict = {\n",
    "            \"author\": author_embedding,\n",
    "            \"paper\": paper_embedding,\n",
    "        }\n",
    "\n",
    "        # \"x_dict\" now holds feature matrices of all node types\n",
    "        # \"edge_index_dict\" holds all edge indices, i.e. the connections between users and movies\n",
    "        # The GNN will predict new embeddings for all node types, we can even check how the user embeddings change\n",
    "        gnn_pred = self.gnn(x_dict, data.edge_index_dict)\n",
    "\n",
    "        # Finally we can use the classifier to get the final link predictions\n",
    "        # This can be done either with the dot product of the updated embeddings\n",
    "        # or more involved with a linear projection head or smth similar\n",
    "        cls_pred = self.classifier(\n",
    "            gnn_pred[\"author\"],\n",
    "            gnn_pred[\"paper\"],\n",
    "            data[\"author\", \"writes\", \"paper\"].edge_label_index,\n",
    "        )\n",
    "\n",
    "        return cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea251cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 8/601 [00:00<00:07, 78.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 82.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.5046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 82.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 82.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 84.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.4413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:06<00:00, 86.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.4336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.4259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 84.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.4246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 84.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 84.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.4135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss: 0.4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss: 0.4069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss: 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss: 0.3983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 84.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss: 0.3987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 84.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss: 0.3966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss: 0.3903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 84.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss: 0.3902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601/601 [00:07<00:00, 83.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss: 0.3913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "EPOCHS = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Model(hidden_channels=256, data=data)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "    for sampled_data in tqdm.tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        sampled_data.to(device)\n",
    "        \n",
    "        y_pred = model(sampled_data)\n",
    "        y_true = sampled_data[\"author\", \"writes\", \"paper\"].edge_label\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * y_pred.numel()\n",
    "        total_examples += y_pred.numel()\n",
    "        \n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ab7387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Test set...\n",
      "Precision: 0.8437\n",
      "Recall: 0.6208\n",
      "F1 Score: 0.7153\n",
      "Accuracy: 0.8353\n",
      "--------------------------------------------------\n",
      "Evaluating on validation set...\n",
      "Precision: 0.8489\n",
      "Recall: 0.5969\n",
      "F1 Score: 0.7010\n",
      "Accuracy: 0.8302\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(data)\n",
    "\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_true = data[\"author\", \"writes\", \"paper\"].edge_label.cpu().numpy()\n",
    "\n",
    "    # binary thresholding at 0.5\n",
    "    y_pred = (y_pred >= 0.5)\n",
    "            \n",
    "    FP = ((y_true == 0) & (y_pred == 1)).sum().item()\n",
    "    TP = ((y_true == 1) & (y_pred == 1)).sum().item()\n",
    "    FN = ((y_true == 1) & (y_pred == 0)).sum().item()\n",
    "    TN = ((y_true == 0) & (y_pred == 0)).sum().item()\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "\n",
    "test_data.to(device)\n",
    "precision, recall, f1_score, accuracy = evaluate_model(model, test_data)\n",
    "print(\"Evaluating on Test set...\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"--------------------------------------------------\")\n",
    "val_data.to(device)\n",
    "precision, recall, f1_score, accuracy = evaluate_model(model, val_data)\n",
    "print(\"Evaluating on validation set...\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58534c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(val_data)\n",
    "\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = val_data[\"author\", \"writes\", \"paper\"].edge_label.cpu().numpy()\n",
    "\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"Validation AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
